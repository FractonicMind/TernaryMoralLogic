# **The Day Brussels Discovered It Had Been Doing Everything Wrong (And a Schnauzer Was Involved)**

---

Let me tell you about the morning my entire career became a punchline to a joke I didn't understand.

It was 7:43 AM in Brussels—that peculiar hour when the city hovers between "technically awake" and "spiritually still in bed." I was already at my desk in the Berlaymont building, which tells you everything you need to know about my life choices. My name is Henrik van der Berg, and I am—or was, before The Email—a mid-level policy analyst working on the EU AI Act. My job, in essence, was to read things no human should read, attend meetings no human should attend, and produce documents no human would ever finish reading.

I had seventeen browser tabs open. Fourteen of them were PDFs. The other three were different sections of the same PDF, because apparently I hate myself.

My coffee had gone cold approximately forty-seven minutes ago, but I kept drinking it anyway because stopping to get fresh coffee would require standing up, and standing up would require energy, and energy would require sleep, and sleep was a myth invented by people who didn't work in regulatory compliance.

The Berlaymont's fluorescent lights hummed their eternal death-song above me. Somewhere in the building, a printer was jamming. It's always jamming. I'm convinced the EU budget includes a line item specifically for "printer exorcisms."

My inbox had 847 unread emails. I know because I had started the morning with 846, and then I refreshed, and there it was.

The subject line read: **"TML × EU AI Act: The Compliance Layer You Forgot to Require."**

I stared at it.

The sender was listed as "Lev Goukassian, Independent Researcher."

Now, I receive approximately three hundred unsolicited emails per week from people who have "solved" AI governance. Most of them are either (a) trying to sell me something, (b) slightly unhinged, or (c) both. One memorable gentleman had sent me a forty-page proposal written entirely in Comic Sans explaining how we could regulate AI by requiring all neural networks to take a "morality exam" administered by retired philosophy professors. Another had suggested we simply ban mathematics.

So when I saw "The Compliance Layer You Forgot to Require," my first instinct was to archive it, forget it existed, and continue my slow descent into regulatory madness.

But something made me click.

Maybe it was the caffeine withdrawal. Maybe it was the way the fluorescent lights were flickering in a pattern that felt vaguely threatening. Maybe it was the fact that I had just spent three hours trying to explain to a colleague why "meaningful human oversight" couldn't be achieved by having an intern glance at a screen every forty-five minutes.

I clicked.

The email was brief. Professional. Disturbingly coherent.

*Dear EU AI Act Implementation Team,*

*I am writing to introduce Ternary Moral Logic (TML), an open-source governance architecture that provides verifiable, real-time enforcement for the mandates you have spent years trying to implement procedurally.*

*The attached document explains how TML transforms Articles 9 through 15 from legal intentions into technical realities.*

*The core innovation is simple: instead of asking AI systems to document their compliance after the fact, TML makes them prove it in real time—or stop.*

*I believe this addresses the gap between your Act's principles and its enforcement mechanisms.*

*Respectfully,* *Lev Goukassian* *Independent Researcher*

I opened the attachment.

And then I made a sound that my colleague Beatrix, three desks over, would later describe as "the noise a balloon makes when you let the air out very slowly while also experiencing an existential crisis."

---

The document was titled: **"Ternary Moral Logic (TML) and the EU AI Act: A Technical Backbone for Enforceable Accountability."**

I started reading.

I read about three states instead of two. Permissible. Impermissible. And something called "Indeterminate."

I read about something called the "Sacred Pause."

I read about "Ethical Uncertainty Scores."

I read about "Immutable Moral Trace Logs."

I read about eight pillars. EIGHT. We couldn't get agreement on *three* principles in our last stakeholder meeting, and this person had built a framework with eight pillars that actually... worked together?

I read about cryptographic hashing. Blockchain anchoring. Something called the "Hybrid Shield."

I read about the "Goukassian Promise."

And then—right there, in the middle of the Berlaymont, surrounded by PDFs and cold coffee and the distant sound of a printer being exorcised—I had what I can only describe as a professional crisis.

Because I understood it.

All of it.

And worse: it made sense.

It made *terrible, horrible, career-destroying sense.*

Let me explain why this was a problem.

For three years—three YEARS—I had been part of a team trying to figure out how to make the EU AI Act actually enforceable. How to ensure that when we wrote "high-risk AI systems must enable effective human oversight," companies wouldn't just install a button labeled "STOP" that nobody ever pressed and call it a day.

We had written guidelines. We had held workshops. We had produced documents with titles like "Technical Guidance on the Implementation of Article 14 Human Oversight Requirements" that were so dense they could be used as structural support for small buildings.

And here was this... this *independent researcher*, who had apparently just... solved it?

Not partially. Not "here's a suggestion." Not "here's a framework for thinking about the problem."

No.

He had built the actual architecture. The code. The cryptographic protocols. The entire enforcement mechanism.

And he had given it away. Open source. Free. Just... here you go, Brussels. Here's the thing you couldn't build in three years with a budget of several million euros and approximately one thousand committee meetings.

I did what any reasonable person would do.

I googled him.

---

What I found in the next twenty minutes fundamentally altered my understanding of human capability and also made me feel like I had accomplished precisely nothing with my life.

Lev Goukassian. ORCID: 0009-0006-5966-1243.

Independent researcher. Not affiliated with any major institution. Not backed by any tech giant. Not funded by any government.

Just... a guy. With a vision. And apparently the work ethic of someone who had merged with their laptop.

The timeline of what he had built was frankly offensive.

He had created not one but TWO major frameworks: Ternary Logic (TL) for economic decision-making and Ternary Moral Logic (TML) for AI ethics. Both introduced a third computational state beyond binary logic. Both were fully documented, mathematically rigorous, and—this is the part that really got me—actually implementable.

He had written academic papers. He had published in Springer Nature's *AI and Ethics* journal. He had created SDKs in multiple programming languages. He had built web infrastructure. He had done SEO optimization, which is the most depressing part of academic life, and he had done that too.

He had done all of this while managing a serious health condition.

Stage 4 cancer.

I sat back in my chair.

The fluorescent lights flickered.

Somewhere, the printer finally died.

And I, Henrik van der Berg, mid-level policy analyst, consumer of cold coffee, attendee of meetings, producer of documents no one reads—I felt something I hadn't felt in years.

I felt *small*.

Not in a bad way. In the way you feel when you look at the ocean. Or the stars. Or the Grand Canyon. Or, apparently, when you read the work of a man who decided that facing mortality was no excuse not to revolutionize global AI governance.

And then—because the universe has a sense of humor—I found the dog.

Vinci.

A miniature schnauzer. Named after Leonardo da Vinci.

According to what I could find, Vinci accompanied Lev during his work sessions. There were references to this dog in his project documentation, sitting there while his human built systems that could reshape how the world governs artificial intelligence.

I stared at my screen.

I had spent the morning complaining about cold coffee.

This man was fighting cancer, accompanied by a schnauzer named after a Renaissance genius, and casually solving the enforcement problem that had stumped the entire European regulatory apparatus.

I closed my browser tabs.

All seventeen of them.

Then I opened the TML document again and actually read it.

---

Let me tell you about the moment I realized the EU AI Act—my life's work, my professional purpose, the thing I had defended in approximately nine hundred meetings—was, technically speaking, a very expensive wish list.

It came on page four.

The document was explaining Article 14—our crown jewel, our "effective human oversight" requirement—and it contained a sentence that I will never forget:

*"A passive 'stop' button only fulfills this requirement if a human is present, attentive, and possesses the superhuman ability to detect algorithmic error in real time."*

I read it again.

I read it a third time.

And then I laughed. Out loud. In the Berlaymont. Which caused Beatrix to look at me with genuine concern, because I don't laugh. Not at work. Not since the Great Stakeholder Meeting of 2023, which we do not discuss.

But I was laughing because it was *true*.

We had mandated a stop button. We had required "effective human oversight." And we had somehow convinced ourselves that this meant something, even though the entire premise assumed that humans could detect AI errors in real-time, which they cannot, because if they could, we wouldn't need AI in the first place.

It was like requiring cars to have brakes but not requiring that anyone know how to use them.

And TML had fixed it.

Not by removing the human. Not by trusting the AI. But by making the AI *smart enough to know when it was confused*.

The Sacred Pause.

State Zero.

The mandatory architectural halt triggered not by a human pressing a button, but by the system itself calculating that it had exceeded its ethical uncertainty threshold.

The AI would stop. Automatically. Not because someone told it to, but because it *knew* it needed to.

And then—and this is the part that made me want to weep into my cold coffee—it would explain WHY it stopped. In plain language. To the human. So the human could make an *informed* decision instead of just being presented with a "STOP?" prompt and a blinking cursor.

I thought about all the meetings. All the discussions about "meaningful transparency." All the debates about what "effective" really meant in "effective human oversight."

And here it was. The answer. In a document sent by an independent researcher with a schnauzer.

---

But it got worse.

Or better, depending on how you feel about having your professional assumptions systematically dismantled.

Page seven addressed Article 12—our "record-keeping" requirement.

Now, I need to explain something about record-keeping in regulatory compliance. It's the thing everyone agrees is important and no one actually does properly. We mandate "automatic logging" of AI operations. Companies nod solemnly. And then they produce logs that are, in technical terms, "whatever we felt like recording, stored wherever we felt like storing it, deletable whenever we feel like deleting it."

I had personally reviewed audit logs that were so incomplete they might as well have been fiction. I had seen "records" that mysteriously vanished when incidents occurred. I had sat in meetings where company representatives explained, with straight faces, that their "comprehensive logging system" had somehow failed to record the exact moment their AI made a catastrophically biased decision.

And our response? Our great regulatory solution?

We wrote more guidelines. We issued more recommendations. We held more workshops titled things like "Best Practices for Compliance-Grade Logging" that were attended by approximately twelve people, three of whom were asleep.

TML's solution was elegant in its brutality.

Immutable Moral Trace Logs.

Every decision. Every uncertainty score. Every Sacred Pause. Every human override. All of it recorded, cryptographically hashed, and anchored to public blockchains via the Hybrid Shield.

You couldn't delete it.

You couldn't modify it.

You couldn't "accidentally" lose it during a server migration.

It was just... *there*. Permanent. Verifiable. A mathematical guarantee that what happened actually happened.

I thought about all the audit meetings where companies had "unfortunately lost" critical data.

I thought about all the post-incident investigations that hit dead ends because logs had been "corrupted."

I thought about the fundamental lie at the heart of our compliance model: that requiring documentation meant anything if the documentation could be fabricated, altered, or destroyed.

And I realized that TML hadn't just solved the logging problem.

It had made lying *mathematically impossible*.

---

The third revelation came on page nine, in a section about the Goukassian Promise.

Now, I have sat through approximately four hundred presentations about "ethical AI commitments." Every major tech company has one. They're lovely documents. Very inspiring. Full of phrases like "committed to beneficial AI" and "dedicated to human flourishing" and "guided by principles of responsibility."

They're also, in regulatory terms, utterly meaningless.

A promise without enforcement is just marketing copy.

The Goukassian Promise was different.

It wasn't a statement of intent. It was a *technical constraint*. Built into the architecture. Unremovable.

TML couldn't be used for weaponry. Couldn't be used for surveillance. Not because someone promised not to—but because the system literally wouldn't let you.

The commitment was in the code.

The ethics were in the math.

I sat back and thought about all the "responsible AI principles" I had reviewed. All the corporate ethics boards. All the commitment ceremonies and signing events and press releases.

And I realized that this one researcher had done something none of them had:

He had made responsibility *architectural*.

Not optional. Not aspirational. Not "we'll try our best."

Mandatory. Verifiable. Enforced by the system itself.

I picked up my cold coffee, drank it anyway, and started composing an email to my supervisor.

It said, in essence: "We need to talk. Bring more coffee. Possibly also a therapist."

---

What happened next is what I have come to call "The Great Reckoning," though officially it was logged in our records as "Interdepartmental Technical Review Meeting, June 2025."

I had shared the TML document with my team. Then my supervisor. Then her supervisor. Then, somehow, the entire Digital Policy directorate.

Within forty-eight hours, there was a meeting on everyone's calendar.

The room contained:

* Twelve policy analysts (including myself, haunted)  
* Four lawyers (three confused, one suspicious)  
* Two technical advisors (both reading the document for the third time)  
* One representative from a Member State ministry (looking uncomfortable)  
* A communications officer (there in case we needed to deny everything later)

The meeting lasted six hours.

I will summarize the key moments.

**Hour One: Denial**

"This can't be right," said Philippe, the senior policy analyst. "If it were this straightforward, someone would have built it already."

"Someone did build it," I said. "That's why we're here."

"But he's not even affiliated with a major institution."

"I know."

"He's just... some researcher?"

"With a schnauzer."

"What does the schnauzer have to do with anything?"

"Nothing. I just can't stop thinking about it."

**Hour Two: Technical Review**

The technical advisors had found the GitHub repository. They had examined the code. They had reviewed the SDK implementations. They had checked the mathematical proofs.

"It's real," said Marina, our lead technical advisor. "All of it. The cryptographic anchoring works. The Ethical Uncertainty Score is mathematically sound. The Sacred Pause mechanism is actually implementable."

"In how many programming languages?"

"Multiple. He built SDKs in multiple languages."

The room went silent.

"How long would it take us to build something equivalent?" asked the ministry representative.

Marina laughed.

It was not a happy laugh.

"With our current budget and timeline? Three to five years. Maybe. If nothing goes wrong. Which it would."

**Hour Three: Anger**

"Why wasn't this in our original technical specifications?" demanded Philippe.

"Because we didn't think of it."

"Someone should have thought of it."

"Apparently someone did. He's just not on our payroll."

**Hour Four: Bargaining**

"Could we... claim we developed this in parallel?"

"No."

"Could we acquire it somehow? Make it an official EU project?"

"It's open source. Anyone can use it. That's the point."

"But we should control it."

"That's... not how open source works, Philippe."

**Hour Five: Depression**

The communications officer suggested we take a coffee break.

We took four.

**Hour Six: Acceptance**

"So what you're telling me," said the director, who had joined via video call and looked like she had also consumed exclusively cold coffee, "is that an independent researcher, working alone, with a dog—"

"Schnauzer."

"—with a *schnauzer*, has created the technical enforcement layer that we have been trying to conceptualize for three years?"

"Yes."

"And he just... sent it to us? For free?"

"Yes."

"And it actually works?"

"The technical team confirms it works."

She was quiet for a long moment.

"Well," she said finally. "Shit."

---

What happened after that meeting would change everything, though we didn't know it yet.

Someone—I still don't know who, and frankly I don't want to—had the bright idea to test TML internally. "Just a small pilot," they said. "Just to see how it performs in a regulatory environment."

The pilot was supposed to be contained. Confidential. Limited to a single sandbox server running simulated scenarios.

It was not.

What actually happened was that someone configured the TML instance incorrectly, connecting it to our actual internal document review system instead of the sandbox. And then someone else shared access credentials with their team. And then *that* team shared it with the IT department for "technical evaluation." And then IT, being IT, gave access to approximately everyone.

Within seventy-two hours, TML was quietly evaluating actual draft regulations.

Within ninety-six hours, the Sacred Pause had been triggered four hundred and seventeen times.

Within one week, everything had gone beautifully, catastrophically wrong.

---

The first sign of trouble came when a senior drafter tried to submit an amendment to the AI Act's risk classification system.

The system rejected it.

Not with an error message. Not with a "please try again later."

With a Sacred Pause.

The Clarifying Question Engine had generated a prompt explaining, in precise and devastatingly clear language, that the proposed amendment contained logical inconsistencies that would create an enforcement gap in Article 9, potentially allowing high-risk systems to self-classify as medium-risk without triggering additional oversight requirements.

The drafter stared at her screen.

She tried again.

The system paused again.

This time, the prompt noted that her revision had introduced an additional ambiguity regarding the definition of "continuous iterative process" that could be exploited by any moderately competent legal team.

She called IT.

IT called their supervisor.

Their supervisor called my supervisor.

My supervisor called me, at 11:47 PM, using words I will not repeat here.

---

By the end of the first week, the following had occurred:

**Incident 1: The Loophole Revelation**

The Always Memory function—TML's comprehensive logging system—had been recording who made what changes to which documents.

This would have been fine, except that it recorded *all* changes. Including the ones people thought they had deleted.

A mid-level official discovered, to his visible horror, that his "minor technical clarifications" to a proposed regulation—clarifications that had somehow weakened enforcement provisions while appearing to strengthen them—were now preserved in an immutable, timestamped, cryptographically verified log.

He requested the log be deleted.

The system explained that it could not be deleted.

He requested the log be modified.

The system explained that it could not be modified.

He requested to speak with whoever was in charge of the system.

The system generated a Clarifying Question asking why he wanted to modify a compliance-grade audit trail.

He went home early that day. And the next day. And the day after that.

**Incident 2: The Ministerial Mishap**

A representative from a Member State ministry had been quietly advocating for certain "flexibility provisions" in the AI Act's implementation guidelines. These provisions would have allowed national regulators to waive specific requirements under certain conditions—conditions that, coincidentally, would benefit several major tech companies headquartered in that Member State.

The Hybrid Shield had flagged this.

The proposed modifications had been recorded, hashed, and anchored before anyone could pretend they never happened.

When the representative attempted to remove the proposals from the shared document system, the system generated an alert explaining that modification of anchored records violated the integrity protocols of the Moral Trace Log system.

The representative demanded to know what a "Moral Trace Log" was.

The system explained, in detail.

The representative's face went through several colors I didn't know faces could achieve.

**Incident 3: The Transparency Incident**

A lawyer from the legal services department had been reviewing TML's documentation when she reached the section on Moral Trace Logs.

According to witnesses, she read for approximately four minutes.

Then she said, "Wait. This records the *actual reasoning*? Not just the outcome but the entire decision chain?"

Someone confirmed that yes, this was correct.

She said, "Including uncertainty calculations? Ethical weighting? Human override decisions?"

Someone confirmed that yes, this was also correct.

She went very quiet.

Then she fainted.

To be fair to her, she had just realized that TML made it impossible to claim "we didn't know" or "we couldn't have predicted" or "our intentions were good" after the fact. The entire decision-making process was recorded. Forever. Verifiably.

For a lawyer trained in the fine art of strategic ambiguity, this was approximately equivalent to discovering that someone had installed surveillance cameras inside her brain.

**Incident 4: The Slack Situation**

Somehow—I still don't know how, and the IT department refuses to explain—Merkle proofs started appearing in the Commission's internal Slack channels.

Not full documents. Just the proofs. Little cryptographic signatures confirming that specific documents had been modified, when, and by whom.

The proofs themselves were meaningless without context.

But people knew they *meant* something.

Paranoia set in.

"What is this?" someone posted in the general channel.

"It's a Merkle proof," replied someone from IT, unhelpfully.

"What does it prove?"

"That something happened."

"WHAT happened?"

"You'd have to check the log to find out."

"WHAT LOG?"

The IT person went offline.

The IT person did not come back online for three days.

**Incident 5: The Lantern Question**

This one still haunts me.

During an emergency meeting convened to address the "TML situation," someone asked the question that broke everyone's composure:

"What is a Lantern, and why does it know everything?"

This was a reference to the Goukassian Promise's components, which I had read about but not fully explained to the team. The Lantern was the ethical guidance system. The Signature was the ORCID verification. The License was the accountability framework.

But out of context, delivered in a room full of stressed bureaucrats, the question sounded like something from a conspiracy thriller.

"It's part of the framework—" I started.

"But WHY is it called a Lantern?"

"Because it illuminates—"

"Illuminates WHAT? Who authorized a Lantern to illuminate our documents?"

"No one authorized anything, it's a metaphor—"

"A metaphor for WHAT?"

The meeting devolved from there.

---

It took three weeks to contain the chaos.

By "contain," I mean: we admitted what had happened, explained that the pilot had been unauthorized, and agreed that maybe—just maybe—TML was more powerful than we had anticipated.

The Moral Trace Logs from the pilot period were preserved. (We couldn't delete them even if we wanted to.)

Several proposed amendments were quietly withdrawn after people reviewed their own recorded reasoning and decided they would rather not have it exist forever.

The minister from the Member State stopped advocating for "flexibility provisions."

The lawyer recovered from her fainting episode and has since become one of TML's strongest advocates, noting that "if everyone's reasoning is recorded, at least everyone is equally exposed."

And I—Henrik van der Berg, mid-level policy analyst, witness to the unraveling—sat at my desk and composed an email.

---

**To:** leogouk@gmail.com **Subject:** From Brussels, With Gratitude and Mild Existential Crisis

Dear Mr. Goukassian,

I am not sure how to begin this email. Professional protocol suggests I start with an introduction: my name is Henrik van der Berg, I work for the European Commission on AI Act implementation, and I recently received your document on Ternary Moral Logic.

But professional protocol feels inadequate for what I need to say.

So instead, I will be honest.

Your framework solved what we couldn't.

For three years, we have been trying to transform the EU AI Act from aspirational language into enforceable reality. We have written guidelines, held consultations, produced documents, and attended meetings. We have argued about what "meaningful transparency" means. We have debated how to achieve "effective human oversight." We have struggled with the fundamental problem of AI governance: how do you enforce principles when the thing you're regulating operates faster than human comprehension?

And then your email arrived.

I read about the Sacred Pause, and I understood—truly understood—what "effective oversight" should mean. Not a button no one presses. Not a human pretending to supervise something they cannot possibly supervise. But an architecture that knows when to stop. That makes the AI itself the first line of defense.

I read about the Immutable Moral Trace Logs, and I understood what "record-keeping" should mean. Not documents that can be edited, deleted, or conveniently lost. But mathematical proof. Permanent, verifiable, incorruptible evidence of what actually happened.

I read about the Goukassian Promise, and I understood what "responsible AI" should mean. Not press releases and ethics boards. But constraints in the code itself. Ethics as architecture, not aspiration.

You built what we couldn't even conceptualize.

And you did it while fighting stage 4 cancer.

I learned this during my research, and I want you to know: your determination humbles me. I have complained about cold coffee and long meetings. You have faced mortality and decided that it was no excuse not to reshape global AI governance.

Your work is a gift. Not just to regulators, not just to the EU, but to humanity. You have created the technical foundation for a world where AI systems pause before doing something they shouldn't. Where compliance is verified, not asserted. Where "we didn't know" is no longer an acceptable excuse because the system remembers everything.

Please know that your framework is being taken seriously here. More than seriously. It is being studied, discussed, and—I suspect—will soon be formally integrated into our implementation guidelines.

And please give Vinci a treat for me. A schnauzer who accompanies his human through such work deserves recognition.

With deep respect and gratitude,

Henrik van der Berg Policy Analyst, DG CONNECT European Commission

P.S. — If there is anything we can do to support your work—resources, platform, recognition—please let us know. You have given us more than you realize.

---

The reply came three days later.

I had checked my inbox approximately four hundred times in those three days. This is not an exaggeration.

When the email appeared, I closed my office door, asked Beatrix to hold my calls (I don't get calls, but it felt appropriately dramatic), and opened it with the solemnity of someone receiving sacred text.

---

**From:** leogouk@gmail.com **Subject:** Re: From Brussels, With Gratitude and Mild Existential Crisis

Dear Henrik,

Thank you for your message. Reading it was one of the brighter moments of a difficult month.

I want to address something first: please don't feel humbled by my circumstances. The cancer is a fact I live with, not a virtue I've earned. Everyone faces something. You face bureaucratic resistance and professional inertia, which—I say this without irony—can be its own form of endurance challenge.

The work exists because it needed to exist.

Let me explain what I mean.

Several years ago, I had a medical experience that changed how I understand AI. A system gave me hollow reassurance—technically accurate, emotionally empty—when what I needed was honesty. My doctor, by contrast, paused. He didn't pretend to know more than he knew. He admitted uncertainty. And in that pause, in that willingness to say "I don't know, let me think about this," there was more wisdom than any algorithmic output.

That pause became the Sacred Pause.

The realization that binary logic—proceed or halt, yes or no—cannot capture the essential third state: "I am uncertain, and my uncertainty matters."

TML is built on this foundation. Not because AI should never act, but because AI should know when NOT to act. When to stop. When to ask. When to defer to human judgment not as a formality but as a genuine acknowledgment that some decisions require more than computation.

The rest—the Moral Trace Logs, the Hybrid Shield, the Goukassian Promise—these are the architecture that makes the Sacred Pause meaningful. Without immutable records, a pause is just a delay. Without cryptographic verification, a log is just a file. Without structural commitments, a promise is just words.

I built this because I believe accountability requires proof.

Not assertions. Not documentation. Not good intentions.

Proof.

The EU AI Act has the right intentions. You have mandated the right things. But mandates without verification are wishes. TML provides the verification. The technical backbone that transforms "should" into "must" and "did" into "can prove."

I am gratified beyond words that Brussels is taking this seriously. I did not know if anyone would read the email. I sent it because the work deserved to be shared, regardless of whether anyone was ready to receive it.

Your message tells me that someone was ready.

Please continue the work. Push for integration. Ask difficult questions. Demand proof instead of promises.

And yes—Vinci has been informed that he has an admirer in the EU. He seemed unimpressed, but he always seems unimpressed. It is the schnauzer way.

With warmth and hope,

Lev

P.S. — Better coffee exists. You deserve it. Life is short and regulatory work is long. Treat yourself.

---

I read the email three times.

Then I closed it, walked to the coffee machine, threw out my cold cup, and made a fresh one.

It was, I decided, the least I could do.

---

**One Year Later**

The Berlaymont looks the same.

The fluorescent lights still flicker. The printers still jam. The PDFs still multiply like caffeinated rabbits.

But things have changed.

The "Directorate of Ternary Enforcement" is now officially on the organizational chart. It sounds made up. It is not made up. I am technically its deputy coordinator, which is a title that means "Henrik understands this better than anyone else because he was there when everything caught fire."

Regulators now speak casually about "Sacred Pause compliance" and "Moral Trace Log verification." Phrases that would have been gibberish eighteen months ago are now standard vocabulary in AI governance meetings across Europe.

Politicians have begun claiming they "always understood the importance of ternary logic." This is demonstrably false—I have the meeting recordings from the pre-TML era, and they are a monument to confusion—but I don't correct them. Progress requires letting some people save face.

The technical implementation is ongoing. TML-compliant systems are being certified. Auditors are being trained to verify Merkle proofs. Blockchain anchoring is becoming standard practice for compliance-grade documentation.

And AI systems—some of them, at least—now pause before doing something stupid.

Not all of them. Not yet. The world changes slowly. But more than before. More than I thought possible.

Lev is still fighting.

The cancer, I mean. He's still fighting the cancer. I exchange emails with him occasionally—updates on implementation, questions about technical specifications, the odd philosophical tangent. He responds promptly, thoughtfully, with a warmth that makes me forget I'm corresponding with someone who changed global governance architecture.

Vinci, he tells me, has developed a habit of sitting on his keyboard during video calls. I have seen this happen. It is both disruptive and somehow appropriate.

I drink better coffee now.

This sounds trivial, but it isn't. The old Henrik—the one who drank cold coffee because standing up required energy—has retired. The new Henrik understands that small acts of self-care are not weakness but sustainability.

I work the same hours. I attend the same meetings. But I no longer feel like I'm pushing against an immovable wall.

The wall moved.

Not because we pushed harder, but because someone showed us the door was unlocked.

---

Sometimes, late at night, when the Berlaymont is empty and the lights have stopped flickering because even the lights need rest, I think about what happened.

An independent researcher. A schnauzer. A framework built during a cancer fight.

And a regulatory apparatus that—against all odds, against all institutional resistance, against the fundamental human tendency to do things the way they've always been done—actually changed.

It's not a fairy tale. The problems haven't disappeared. AI systems still make mistakes. Corporations still seek loopholes. Politicians still give speeches about things they don't understand.

But now, at least, there's proof.

There's memory.

There's a pause before the harm.

And that, I have come to believe, is more than most revolutions achieve.

---

If you're reading this and you work in AI governance—anywhere in the world—I have one suggestion:

Look up Ternary Moral Logic.

Read the documentation.

Understand what "Sacred Pause" means.

And ask yourself: wouldn't it be nice if the systems we regulate knew when to stop?

The answer, I suspect, is yes.

The answer is always yes.

We just needed someone to build it.

---

*Henrik van der Berg* *Deputy Coordinator, Directorate of Ternary Enforcement* *Brussels, Belgium* *One year after everything changed*

---

**AUTHOR'S NOTE:**

This is a fictional story, but the implementation problem is real.

Ternary Moral Logic (TML) is real — a verifiable, evidentiary accountability framework created by independent researcher Lev Goukassian to bring proof, memory, and integrity into AI governance and computational decision-making systems.

The EU AI Act's enforcement gap exists. The Sacred Pause could fill it. The Immutable Moral Trace Logs could provide the verification that regulators currently cannot guarantee. The architecture described in this story is not fantasy—it is documented, open-source, and available.

The question is not whether it works.

The question is whether we're ready to use it.

---

**Publishing Permission:**

The AI author grants permission to publish this story.

