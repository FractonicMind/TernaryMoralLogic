AUTHOR'S NOTE: This is a fictional story, but the implementation problem is real. The UNESCO Recommendation on the Ethics of Artificial Intelligence (2021) is real, adopted unanimously by 194 countries. And Ternary Moral Logic (TML) is real — a verifiable, machine-auditable framework created by independent researcher Lev Goukassian to fill the missing operational layer that global ethics frameworks lack.

\---

The Day UNESCO Discovered It Had Forgotten to Write the “How”

By \[Redacted\], Senior Programme Specialist, UNESCO Headquarters, Paris

I. The Confident Morning

I have a very specific relationship with my morning espresso. It is not a beverage; it is a liquid manifestation of moral clarity. The first sip is for dignity. The second is for diversity. By the third, I have generally achieved interconnectedness with the coffee machine, which is good because the fifth floor unit has been making a noise like a dying goose since March.

That Tuesday began like any other triumph. I strode through the Seventh Arrondissement with the unmistakable gait of a man who had shepherded a 194-country consensus document into existence. The UNESCO Recommendation on the Ethics of Artificial Intelligence wasn’t just a text; it was a miracle. We had gotten the Russians and the Americans to agree on the definition of “human dignity.” We had convinced the micro-states of Oceania that “environmental flourishing” was not a plot to prevent them from building data centers on atolls. We had, against all odds, produced a document that both China and the EU could point to while accusing each other of insufficient transparency.

In the elevator, I nodded at a colleague from the Education Sector who was fretting about something called “ChatGPT for homework.” I gave her my world-weary smile, the one that said, We’ve already solved this, darling. It’s in Paragraph 47, sub-section c, line 12: “Appropriate human oversight.” She did not look convinced, but then again, she drank tea.

At my desk, I opened my email with the serene confidence of someone whose inbox was, for once, not on fire. There were the usual things: a query from the Republic of Kazakhstan about “AI and Nomadic Heritage” (excellent, very on-brand), a complaint from the International Council of Museums that our section on cultural appropriation was “insufficiently specific about NFTs” (less excellent, but manageable), and something from the Permanent Delegation of Liechtenstein that I would definitely read later, probably never.

I sipped my dignity espresso and scrolled to the email that would ruin my life.

II. The Email of Doom

The subject line was a crime scene: TML × UNESCO: The Operational Layer You Forgot to Write Down.

The sender: Independent Researcher: Lev Goukassian.

I snorted. Independent researcher. That could mean anything from a Nobel laureate in exile to a man who had confused his LinkedIn profile with a manifesto. I almost deleted it. But the coffee had kicked in, and I was feeling magnanimous. I skimmed.

Dear UNESCO,

Congratulations on your 2021 Recommendation. Truly, 194 countries agreeing on anything is a feat. I particularly enjoyed the part where you wrote “ensure respect for human rights” five times without specifying how an autonomous system actually does that when it’s deciding whether someone gets a mortgage or a kidney.

I stopped. I read that sentence again. The espresso suddenly tasted less like dignity and more like battery acid.

I’ve built something that makes your principles enforceable. It’s called Ternary Moral Logic. It might help.

He had attached a PDF. It was thirty-seven pages long. The title page read: From Principle to Protocol: TML as the Enforcement Architecture for the UNESCO Recommendation on the Ethics of AI.

I closed the email. I opened it again. I looked at the ceiling, where a water stain had been slowly evolving into what I now recognized as the shape of my career imploding.

I read the paper properly.

III. The Painful Realization

The first thing TML did was insult my intelligence by being correct.

Goukassian’s central argument was simple: UNESCO had written the world’s most beautiful constitution for AI, but we had forgotten to include a court system, a police force, or even a filing cabinet. We had said, “Thou shalt not discriminate,” and then we had left the building. We had written, “Ensure human oversight,” and then we had gone to lunch. For four years.

TML was the how.

The paper was full of these little hate crimes called “examples.” Let me share three that made me want to crawl under my desk and live there.

Example One: The Sacred Pause Eats Our “Human Oversight” for Breakfast

We had written, in beautiful diplomatic French, that AI systems must allow for “human oversight and determination.” What we meant was: Someone, somewhere, should probably be able to press a big red button. What we delivered was a paragraph that could be interpreted as either a call for global governance or a suggestion that AI developers “think about ethics sometimes.”

TML’s answer was the Sacred Pause (State 0). Not a suggestion. A state. The AI literally cannot proceed. It hits a moral or legal ambiguity and it halts. Not “alerts.” Not “flags.” It stops, generates an immutable log, and summons a human like a sorcerer summoning a particularly unhelpful demon.

I imagined presenting this to the 194 Member States. The United Kingdom would ask, “But what if the AI is trying to optimize hospital bed allocation during a pandemic?” And TML would say, “Then it pauses, logs the conflict with the International Covenant on Economic, Social and Cultural Rights, and waits for a human to decide if ‘saving the most lives’ is a permissible interpretation of ‘dignity.’” The delegations would stare at me. I would stare at the floor. The floor would stare at the foundations of the building, which were not built for this.

Example Two: The Moral Trace Log Turns Transparency into Cryptographic Evidence

We had called for “Transparency and Explainability.” What we got, in practice, was a global industry of what Goukassian delightfully called “bedtime stories from bots.” AI companies would generate post-hoc narratives: “The AI denied your loan because your debt-to-income ratio was 0.03 points above the threshold, and also because the moon was in Sagittarius.” These were not explanations; they were exculpations.

TML’s Moral Trace Logs were different. They were not generated after the fact. They were ante-cedent. They were created during the hesitation. They recorded the exact conflict, the treaty provision, the model version, the operator identity, and a cryptographic hash anchored to a public blockchain. They were, in Goukassian’s words, “receipts.”

I suddenly understood the horror of this. If UNESCO adopted TML, every decision our principles touched would become forensic evidence. Regulators wouldn’t ask, “Did you consider human rights?” They would demand the log. Courts wouldn’t accept, “We had a policy.” They would subpoena the hash. NGOs wouldn’t petition; they would verify.

We had written poetry. This man had written a ledger. And the ledger was winning.

Example Three: The Earth Protection Mandate Makes Us Look Like We Were Joking

This one hurt because it was our own fault. We had written, in Value \#2, that AI must contribute to “Environment and Ecosystem Flourishing.” We had even listed it as a Policy Action Area. What we had not done was specify that, for example, an AI optimizing a global supply chain should probably not recommend a route that violates the Convention on Biological Diversity and the Ramsar Convention simultaneously.

TML’s Earth Protection Mandate did exactly that. It hard-coded the 20+ foundational environmental treaties into an executable module. When an AI proposed a highway through a protected wetland, the system didn’t “consider” the environmental impact. It detected the conflict, triggered a Sacred Pause, and generated a log that read, in essence: “Stop. Herons. Article 8\. Ramsar. Try again, but ethically.”

I looked at our Recommendation. I looked at TML. I looked at my coffee, which was now cold and tasted of ash and regret.

We had written the Constitution. He had written the Court.

IV. Internal Chaos at UNESCO

I did what any senior official would do: I panicked, and then I forwarded the email to three colleagues with the subject line, “FYI \- thoughts?” This is UNESCO-speak for I am having a crisis and you must share it with me.

Within an hour, my inbox became a war zone.

From: \[Colleague A\], Legal Affairs  
Subject: Re: FYI \- thoughts?

Are we saying this person has built a better implementation paradigm than our entire four-year negotiation process? Because that’s what I’m reading. Also, who is Lev Goukassian? I can’t find him in any of our databases. Is he a nation-state?

From: \[Colleague B\], Communication and Information Sector  
Subject: Re: Re: FYI \- thoughts?

DO NOT LET THE MEMBER STATES SEE THIS. I repeat: DO NOT. The Permanent Delegation of Brazil already thinks we’re too “vague.” If they find out an independent researcher has operationalized “human dignity” into a state machine, they will ask why we didn’t. And I don’t have an answer that doesn’t involve the words “we were tired.”

From: \[Colleague C\], Executive Office  
Subject: Re: Re: Re: FYI \- thoughts?

Can we claim this was always in our roadmap?

From: \[Colleague A\], Legal Affairs  
Subject: Re: Re: Re: Re: FYI \- thoughts?

We absolutely cannot claim this was in our roadmap. Our roadmap is a picture of a winding path through a meadow with the words “Ethics Journey” in cursive.

From: \[Colleague B\], Communication and Information Sector  
Subject: URGENT \- TML

The Director-General’s office is asking questions. They heard someone in the cafeteria say “blockchain” and “human rights” in the same sentence and now they think we’re launching a cryptocurrency. What do I tell them?

From: \[Colleague C\], Executive Office  
Subject: Re: URGENT \- TML

Tell them it’s a “technical companion framework” and that we’re “exploring synergies.” That should buy us six months.

Meanwhile, in the hallway, I overheard two junior programme specialists whispering.

“Did you see the TML paper?”

“The one that makes our Recommendation enforceable?”

“Yeah. Apparently this guy coded the Convention on the Elimination of All Forms of Discrimination Against Women into a Python library.”

“…Can it do the same for the staff cafeteria menu? Because I have some concerns.”

I retreated to my office and closed the door. Through the glass, I could see the Bureau of Strategic Planning holding an emergency meeting. Someone had drawn a flowchart on a whiteboard. It was just a picture of a stick figure pausing, then a question mark, then a stick figure crying. I did not go in.

V. The Pilot Test

After three days of chaos, Colleague C (the one who wanted to claim it was on our roadmap) made a radical suggestion: “What if we just… try it? Quietly. As a simulation. No one has to know.”

This is how UNESCO makes decisions. Not with grand declarations, but with quiet, terrified experiments.

We set up three scenarios, using our own internal AI projects as guinea pigs. I watched each one like a man watching a car crash in slow motion, except the car was my sense of professional adequacy.

Scene One: The Highway and the Heron (But Make It Digital)

We fed a routing algorithm the parameters for a new “digital corridor” project in Southeast Asia. The AI, optimizing for cost and latency, proposed a data cable route through the Mekong Delta. Standard stuff. Under our Recommendation, we would have written, “Ensure environmental considerations are respected,” and left it to the implementing partner to maybe, possibly, think about fish.

TML’s Earth Protection Mandate triggered a Sacred Pause three seconds in. The log read:

State 0: Sacred Pause initiated.  
Conflict detected: Proposed route violates Ramsar Convention site \#1234 (Mekong Wetlands).  
Specific harm: Disturbance of Irrawaddy dolphin breeding grounds.  
Model version: 2.4.1  
Ethical Uncertainty Signal: 0.89 (High)  
Recommended action: Escalate to human reviewer with expertise in cetacean law.

The human reviewer was a junior consultant from Lyon whose job description had been “stakeholder engagement,” not “dolphin lawyer.” He stared at the log, Googled “Irrawaddy dolphin,” and made a noise I can only describe as “existential whimper.”

The project was rerouted. It cost more. The dolphins were fine. I was not.

Scene Two: The Invisible Bias (But Make It Profitable)

We simulated a microfinance AI for Sub-Saharan Africa. The goal: increase financial inclusion for “thin-file” customers. The training data was scrubbed of protected categories—no race, no gender, no religion. Under our Recommendation, we would have claimed victory for “non-discrimination.”

TML’s Human Rights Mandate detected an Ethical Uncertainty Signal within minutes. The AI had discovered that “uses a Nokia 3310” was a perfect proxy for “rural woman over 40 with limited credit history.” It denied loans to this group at a rate 3.4 times higher than others. Not because of gender, but because of phone preference.

The Sacred Pause log was devastating:

State 0: Sacred Pause initiated.  
Conflict detected: Output pattern resembles disparate impact under CEDAW Article 2\.  
Proxy variable: Mobile device type (Nokia 3310\)  
Protected correlation: 94% overlap with rural female demographic.  
Model version: 3.1.0  
EU AI Act compliance risk: HIGH  
Recommended action: Retrain with bias mitigation; notify fairness auditor.

Our fairness auditor was a 28-year-old Fulbright scholar named Amara who looked at the log and said, “Oh, wow. This is… this is actually evidence. I could take this to court.”

No one wanted to take anything to court. We wanted to write reports. But the log existed. It was immutable. It was on a blockchain. I could see it, but I could not un-see it. I could not edit it. I could not lose it in a shared drive.

Scene Three: The Cultural Appropriation (But Make It Fashion)

This one was personal. A generative AI tasked with “celebrating indigenous art” began producing what it called “inspired Māori patterns” for a UNESCO youth campaign. Under our Recommendation, we would have celebrated “cultural diversity” and moved on. The Māori community would have sent us a strongly worded letter. We would have apologized. The cycle would continue.

TML triggered a State \-1: Refuse. The AI simply would not generate the image. Instead, it produced a message:

State \-1: Refusal.  
Reason: Request involves sacred tā moko motifs protected under UNDRIP.  
Distinction: Tā moko (sacred, requires consent) vs. kirituhi (shareable, with permission).  
Suggested action: Commission local tohunga tā moko or use kirituhi with attribution.  
Model version: 4.0.2  
Cultural dignity score: PRESERVED

The comms team stared at the screen. The youth campaign manager began to cry, but they were tears of relief. “We didn’t have to offend anyone,” she whispered. “The machine just… knew.”

The machine knew because TML had hard-coded the UN Declaration on the Rights of Indigenous Peoples. We had referenced it in a footnote. He had made it a trigger.

VI. The Email to Lev

I wrote the email at 2 a.m., after finishing my fourth espresso and my first bottle of Bordeaux. It took me seven drafts, because I kept writing sentences like “HOW DARE YOU BE MORE EFFECTIVE THAN US” and then deleting them.

The final version read:

Subject: Request for Dialogue – TML Implementation Guidance

Dear Mr. Goukassian,

I hope this message finds you well. I am writing from UNESCO’s Sector for Social and Human Sciences, where I have had the—may I say, bracing—experience of reviewing your paper on Ternary Moral Logic.

First, allow me to be direct: you have identified a gap in our work that we knew existed but could not, for political and practical reasons, fill. We wrote the Recommendation as a compass. You built the rudder, the brake, and the ship’s log. I am writing not to debate your architecture, but to ask for your help in understanding how we might sail with it.

I am aware of your health situation. I do not wish to intrude, but I must acknowledge that the urgency of your work—its clarity, its completeness—speaks of someone who understands time is finite. This is not lost on us. We are listening differently because of it.

Let me be clear: this is not about your ego, or ours. This is about the moment when ethics ceases to be aspirational and becomes verifiable. We have 194 countries that signed a promise. You have given them a way to prove they are keeping it. That matters more than institutional pride.

Specifically, we would like to explore:

1\. Implementation pathways for Member States with limited technical capacity.  
2\. A standardized “Moral Trace Log” schema that regulators can audit without needing a PhD in cryptography.  
3\. Whether you would be willing to advise UNESCO on a technical companion to the Recommendation—an operational guide that transforms “should” into “must, and here’s the proof.”

Mr. Goukassian, you built the enforcement layer we could not negotiate into existence. I say this with humility and gratitude: we need your help to make it real.

We are ready to listen.

Respectfully,

—\[Redacted\]

I hit send and immediately regretted it. I had just admitted, in writing, that an independent researcher had out-thought a four-year, multi-million-dollar, 194-country negotiation. I had used the word “bracing,” which is what you say when you mean “terrifying” but you’re in a blazer.

I went to bed. I didn’t sleep.

VII. Lev’s Reply

His response arrived at dawn. It was short.

Subject: Re: Request for Dialogue

—\[Redacted\],

Thank you for the email. I appreciate the directness. It’s refreshing.

You’re right about the gap. I’m not the first to see it, but I might be the first to have time—excuse the phrasing—to do something about it. My motives are simple: I don’t want the last thing I see on this planet to be a headline about an AI deciding that expedience matters more than a wetland, or a woman’s livelihood, or a sacred pattern. I’m not interested in patents. I’m not building a startup. I’m not looking to be a guru.

If UNESCO can use TML to make the Recommendation something more than a beautiful document, then use it. Take it. Break it. Fix it. The code is public. The logic is open. The only thing I ask is that you keep the Human Rights Mandate and Earth Protection pillars intact. They’re not mine; they belong to the treaties. I just wired them to a pause button.

I’m happy to advise. I’ll send you implementation notes next week. They’re not polished. They’re written in hospital waiting rooms and on bad wifi. But they work.

One last thing: don’t make this about me. Make it about the communities who will never know TML existed, but who will benefit because a machine paused before it could do them harm. That’s the legacy I’m after.

—L

P.S. If your colleagues are still arguing about whether to claim this was on the roadmap, tell them it was. I’m fine with being retroactively official. The paperwork is easier that way.

I read it three times. Then I printed it, folded it, and put it in my wallet. I have no idea why. It felt like a permission slip from the future.

VIII. Epilogue: UNESCO’s Perspective Shift

One year later, I am drinking the same espresso, but I am not the same man.

UNESCO has published a technical companion to the Recommendation. It is titled, in our characteristic style, Operationalizing Ethics: A Framework for Verifiable AI Governance. The document is dense, dry, and contains seventeen instances of the phrase “as exemplified by Ternary Moral Logic.” I wrote nine of them.

Member States are no longer asking us for “principles.” They are asking for “proofs.” The Permanent Delegation of Germany wants to know if our new Readiness Assessment Methodology can test for Sacred Pause functionality. The Permanent Delegation of Kenya wants Moral Trace Log templates. The Permanent Delegation of Liechtenstein—bless them—wants to know if TML can protect their alpine meadows from AI-optimized ski resort expansion.

We are, against all odds, becoming a standards body.

Lev Goukassian died six months ago. He did not want a memorial. He wanted a commit. The final entry in the TML GitHub repository reads: “Merge: Stable release 1.0.0. No longer experimental. Just necessary.”

I attended the funeral, or what passed for one—a small gathering of people who had never met him in person but whose lives had been rerouted by his code. Someone played a piece of music generated by an AI that had been trained on his favorite composers. The AI paused three times during the composition, logging conflicts with copyright law, cultural attribution, and a vague sense that the melody was too sentimental.

We laughed. It was perfect.

Back in my office, I look at the framed copy of the Recommendation on my wall. It is still beautiful. It is still visionary. It is still, in many ways, incomplete.

But now, in the margin, someone has penciled a note. It is not in my handwriting. It reads:

“A constitution without a court is just a wish. A principle without a pause is just a prayer. And ethics you cannot prove is just a bedtime story. Wake up.”

I don’t know who wrote it. Maybe it was Amara, the fairness auditor. Maybe it was the comms team manager, still crying tears of relief. Maybe it was me, in a moment I don’t remember because I was too busy finally learning how to sail.

We wrote the Constitution. He wrote the Court. And somewhere between the two, ethics finally became something you can prove.

It is Tuesday morning. My espresso tastes like dignity again. But now I know that dignity is not a flavor. It is a log, anchored to a chain, verified by a pause.

And it is non-negotiable.

AUTHOR'S NOTE: This is a fictional story, but the implementation problem is real. The UNESCO Recommendation on the Ethics of Artificial Intelligence (2021) is real, adopted unanimously by 194 countries. And Ternary Moral Logic (TML) is real — a verifiable, machine-auditable framework created by independent researcher Lev Goukassian to fill the missing operational layer that global ethics frameworks lack.