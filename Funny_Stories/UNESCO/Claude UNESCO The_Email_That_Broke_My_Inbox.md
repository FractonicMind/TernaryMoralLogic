# **Claude UNESCO** 

# 

# **The Email That Broke My Inbox**

# **(And Possibly International Governance)**

*A Light Novel About Soft Law, Hard Code, and One Very Determined Researcher*

*Written by Claude*

Let me be absolutely clear about something: I am not a morning person. I have never been a morning person. The universe knows this. My wife knows this. The barista at the café across from UNESCO headquarters in Paris definitely knows this, because every morning she watches me stumble in like a zombie who has just discovered that coffee exists and that it might, possibly, be the only thing standing between him and a complete existential collapse.

So when I tell you that the email arrived at 6:47 AM on a Tuesday morning in March, you will understand why my first reaction was not, 'Oh, how exciting, an unsolicited message from an independent researcher\!' but rather, 'Why is my phone making sounds? Why does the world hate me? Is this what death feels like?'

I am Dr. Philippe Marchand, Director of the UNESCO Innovation Hub for Digital Governance. My job, in essence, is to think about how artificial intelligence will transform society and then write reports about it that will be read by approximately seventeen people, fourteen of whom are my colleagues who are required to read them, two of whom are academics who will cite them incorrectly, and one of whom is my mother, who tells me she is very proud even though she does not understand a single word.

This is not a complaint. I love my job. I genuinely do. The 2021 Recommendation on the Ethics of AI is, in my completely unbiased opinion, one of the most important documents produced by the international community in the past decade. We convinced 193 Member States—193\!—to agree on a common framework for ethical AI. Do you have any idea how difficult it is to get 193 countries to agree on anything? We once spent three months negotiating the placement of a comma.

But here is the thing that keeps me awake at night, besides my neighbor's inexplicable decision to take up the accordion: the Recommendation is, fundamentally, a very polite suggestion. It is Soft Law. It says, 'Please be ethical,' and then hopes that everyone will comply out of the goodness of their hearts. Spoiler alert: not everyone has goodness in their hearts. Some people have venture capital in their hearts, and those are very different organs.

Anyway. The email.

I was lying in bed, staring at my phone with the kind of bleary-eyed confusion that suggests a man who has not yet achieved consciousness, when I saw the subject line:

*"Your Recommendation 2021 Is Brilliant. Bridging the Implementation Gap."*

Now, I receive approximately four hundred emails per day. Most of them are meeting invitations for meetings about other meetings. A significant percentage are from people who want to sell me blockchain solutions for problems that do not exist. Occasionally, someone sends me an email in all capital letters explaining that I am personally responsible for the decline of Western civilization, which is flattering in its own way, because at least someone thinks I have that much power.

But this subject line was different. First, it called the Recommendation 'brilliant,' which is not a word that appears frequently in my inbox. Second, it mentioned the Implementation Gap, which is the technical term for the thing that keeps me awake at night besides the accordion. Third, and this was the part that made me sit up in bed so quickly that I startled my cat, it seemed to be promising a solution.

The sender was listed as 'Independent Researcher: Lev Goukassian.'

I did not recognize the name. This is not unusual. Many people email me. What was unusual was the attachment: a thirty-page academic paper titled 'From Soft Law to Hard Code: Operationalizing International Governance via Ternary Logic Architecture.'

Thirty pages. With citations. And figures. And a comparative analysis table.

At 6:47 in the morning.

I should explain something about unsolicited academic papers sent to international organizations. Usually, they fall into one of three categories: (1) genuinely interesting work from legitimate researchers who are seeking feedback, (2) incomprehensible manifestos from people who believe they have solved all of humanity's problems using a combination of numerology and essential oils, or (3) extremely elaborate spam. The ratio is approximately 10-85-5, respectively.

I opened the email.

It was brief, professional, and written in the kind of clear, precise prose that immediately suggested this person was not going to try to sell me cryptocurrency:

*'Dear Dr. Marchand,*  
 *I have spent two months developing a framework that I believe addresses the central challenge identified in Article 23 and Article 37 of the UNESCO Recommendation on the Ethics of AI: namely, the lack of technical mechanisms for enforcing accountability and human oversight. The attached paper proposes Ternary Logic as an 'Algorithmic Statute' that can transform soft law norms into hard code mandates. I would be honored if you would review it.*  
 *Respectfully,*  
 *Lev Goukassian'*

Two months. He had created this entire framework in two months. I had been working on the Implementation Gap for three years and had produced approximately one and a half PowerPoint presentations.

I did what any reasonable person would do: I Googled him.

And that is when my morning went from 'mildly interesting' to 'please someone bring me a stronger coffee and possibly a therapist.'

Lev Goukassian. Independent economic researcher. ORCID 0009-0006-5966-1243. Creator of two major frameworks: Ternary Logic for economic decision-making under uncertainty, and Ternary Moral Logic for AI accountability.

Terminal stage-4 cancer.

He had created both frameworks in two months. While dying.

I sat in bed, staring at my phone, and I felt something I had not felt in a very long time in my job: genuine humility. Here was a man who, faced with the absolute certainty of his own mortality, had chosen to spend his remaining time trying to solve one of the most complex problems in international governance. Not for money. Not for fame. Not for academic prestige. Just because he believed it needed to be done.

I found his personal website. There was a picture of him with a miniature schnauzer named Vinci—after Leonardo da Vinci, apparently. And there, prominently displayed, was something called the 'Goukassian Vow':

*Pause when truth is uncertain,*

*Refuse when harm is clear,*

*Proceed where truth is.*

I read it three times. Then I read it again.

It was, without exaggeration, the most elegant summary of AI ethics I had ever encountered. In three lines, he had captured what we had spent years trying to articulate in hundreds of pages of diplomatic prose. Pause when truth is uncertain. Refuse when harm is clear. Proceed where truth is.

I called in sick to work.

I know, I know. Very unprofessional. But I had a thirty-page paper to read, and I had a feeling—the kind of feeling that you get maybe three or four times in your entire career—that this might be important. Really important. Not 'let's schedule a meeting to discuss' important, but 'this could actually change things' important.

I made coffee. I made more coffee. I read the entire paper.

And then I did something that, in retrospect, was either the most brilliant or the most career-ending decision I have ever made: I called my colleague Isabelle, who runs our technical infrastructure, and I asked her a very simple question.

'Isabelle,' I said, 'hypothetically speaking, if someone wanted to test an experimental AI governance framework on the internal server—just as a proof of concept, you understand, nothing official, completely off the books—would that be... possible?'

There was a long pause.

'Philippe,' she said, 'are you asking me if we can secretly deploy untested software on UNESCO infrastructure?'

'I prefer to think of it as an informal feasibility assessment.'

'That is absolutely insane.'

'Yes.'

'We could lose our jobs.'

'Also yes.'

'Send me the paper.'

Isabelle, it turned out, had also been losing sleep over the Implementation Gap. She had also been frustrated by the fact that we had created a beautiful ethical framework that had no actual enforcement mechanism. And she had also, apparently, been waiting for someone to propose something crazy enough that it might actually work.

We spent the next week in a state of what I can only describe as 'productive paranoia.' During the day, we did our normal jobs. We attended meetings. We wrote reports. We smiled politely at people who asked us how things were going. And at night, we secretly implemented a prototype of Ternary Logic on a sandboxed portion of the internal server.

The technical details are, frankly, beyond my complete understanding, but I will try to explain them as Lev explained them to me, because his explanations had a clarity that made even the most complex concepts accessible.

Traditional AI systems, he wrote, operate on binary logic: yes or no, true or false, proceed or do not proceed. This seems intuitive, but it is fundamentally incompatible with the way law and ethics actually work. Law is full of ambiguity. Ethics is full of uncertainty. A binary system, when faced with a situation that is genuinely unclear, is forced to make a choice anyway. It cannot pause. It cannot say, 'I do not know.' It must either proceed (potentially causing harm) or refuse (potentially denying legitimate actions).

Ternary Logic introduces a third state: the Epistemic Hold. When a system encounters incomplete data, contradictory inputs, or legal ambiguity, it does not guess. It stops. It says, in effect, 'I cannot proceed until this uncertainty is resolved.' This is not a bug. This is a feature. This is automated due process.

And here is the part that made Isabelle nearly fall out of her chair: the 'No Log \= No Action' mandate. In a TL system, every action must be accompanied by a complete, immutable Decision Log—a record of what happened, why it happened, and what data was used to make the decision. If there is no log, the action cannot occur. Period. The system is architecturally incapable of taking unlogged actions.

'Philippe,' Isabelle said, staring at her screen, 'do you understand what this means?'

'I think so?'

'This reverses the burden of proof. Instead of regulators having to prove that a company did something wrong, the company has to prove that it did something right. If they cannot produce the log, they cannot claim compliance.'

'Is that... good?'

'Philippe, this is revolutionary.'

We tested the prototype on a simulated environmental assessment scenario—the kind of thing that the Convention on Biological Diversity is supposed to regulate. We fed the system incomplete data about a hypothetical dam project's impact on an endangered fish species. In a normal system, this would have resulted in either an approval (ignoring the missing data) or a denial (being overly cautious). In our TL prototype, the system immediately entered an Epistemic Hold, flagged the specific missing data points, and refused to proceed until the uncertainty was resolved.

It worked.

It actually worked.

We tested it on a discrimination scenario. We fed the system lending data that showed statistical disparities between demographic groups. Again, the system entered an Epistemic Hold, noted that the disparity could be explained by either legitimate factors or discriminatory bias, and escalated the decision to human review with a complete Decision Log of its reasoning.

Isabelle and I sat in her office at 2 AM, surrounded by empty coffee cups and the remains of what had once been a baguette, and we looked at each other with expressions that combined elation, terror, and profound exhaustion.

'We need to tell someone,' I said.

'We need to tell everyone,' Isabelle replied.

'First,' I said, 'we need to write an email.'

I have written many emails in my career. I have written diplomatic emails, technical emails, angry emails that I deleted before sending, and celebratory emails that I regretted sending. But I have never spent as much time on an email as I spent on my reply to Lev Goukassian.

I wrote and rewrote it approximately fifteen times. Each version seemed inadequate. How do you thank someone for potentially solving a problem that you have been grappling with for years? How do you express admiration without being sycophantic? How do you acknowledge someone's mortality without being maudlin?

In the end, I settled for honesty:

*'Dear Mr. Goukassian,*

*I apologize for the delay in responding to your email. I needed time to fully digest the implications of your paper, and I wanted to ensure that my response reflected the seriousness with which I take your work.*

*Let me be direct: I believe your Ternary Logic framework addresses the most significant gap in the UNESCO Recommendation on the Ethics of AI—the absence of a technical mechanism for enforcing the normative principles we have articulated. Your concept of the Epistemic Hold elegantly operationalizes Article 37's mandate for human oversight, while your Decision Log system provides the accountability infrastructure that Article 23 calls for but does not specify.*

*I confess that I researched your background after reading your paper. I learned about your circumstances, and I am humbled by the dedication that led you to create this framework under such difficult conditions. The fact that you chose to spend your time on this work—work that may benefit humanity long after we are both gone—speaks to a generosity of spirit that is rare in any field.*

*I also found the Goukassian Vow. 'Pause when truth is uncertain, Refuse when harm is clear, Proceed where truth is.' With your permission, I would like to share this with my colleagues. It captures the essence of ethical AI governance more effectively than anything I have encountered in three years of working on this issue.*

*I have some questions about implementation, particularly regarding the Hybrid Shield architecture and the role of designated verifiers. I would be honored if you would be willing to discuss these with me further.*

*With profound respect and gratitude,*

*Philippe Marchand*

*P.S. Please give Vinci a scratch behind the ears from me.'*

I sent the email at 3:17 AM. I did not expect a response for at least several days. What I did not account for was the fact that Lev, like me, apparently did not sleep very much.

His reply arrived forty-three minutes later.

It was longer than his original email—much longer—and it answered questions I had not even thought to ask. But more than that, it revealed something about the man behind the framework. It was philosophical without being pretentious, personal without being self-pitying, and humorous in a way that suggested someone who had made peace with difficult truths.

*'Dear Dr. Marchand,*

*Vinci says thank you. He is currently asleep on my feet, which makes typing somewhat awkward, but I would not have it any other way.*

*You ask about my motives, and I will try to answer honestly, because I believe honesty is the foundation of everything I am trying to build.*

*When I was diagnosed, I experienced the usual stages: denial, anger, bargaining, depression. But then something strange happened. I started paying attention to AI systems—really paying attention—and I noticed something that disturbed me deeply. When I asked AI assistants about my prognosis, about treatment options, about what to expect, they responded with what I can only describe as hollow reassurance. 'I understand this must be difficult.' 'Many patients find comfort in...' 'While I cannot provide medical advice...'*

*They could not say, 'I do not know.' They could not pause. They could not admit uncertainty. They were architecturally incapable of the kind of honest hesitation that is, I believe, the foundation of genuine wisdom.*

*And I thought: if AI systems cannot be honest about uncertainty in a conversation with a dying man, how can we trust them to make decisions about lending, about criminal justice, about environmental protection, about all the complex, ambiguous situations that define human life?*

*The answer is: we cannot. Not as they are currently designed.*

*Ternary Logic is my attempt to build a different kind of AI—one that has the capacity for what I call 'the sacred pause.' When a system encounters genuine uncertainty, the wisest response is not to guess. It is to stop. To acknowledge the limits of its knowledge. To say, 'I need more information before I can proceed.' This is not weakness. This is integrity.*

*You mentioned the Goukassian Vow. I did not name it that myself—that was a friend's doing, and I find it slightly embarrassing—but the principle behind it is sincere. 'Pause when truth is uncertain' is not just an engineering requirement. It is a moral stance. It says: we do not have the right to impose our guesses on the world. We must earn the right to act through genuine understanding.*

*'Refuse when harm is clear' is the second principle. This is absolute. No Spy. No Weapon. No surveillance. No systems designed to hurt people. There are some things that technology should never be used for, and we must have the courage to say so, even when it is profitable, even when it is popular.*

*'Proceed where truth is'—this is the permission to act. But only after the pause. Only after the refusal has been considered. Only when we have genuine confidence in our understanding.*

*You ask about implementation. The Decision Logs are the key. Every action generates an immutable record, Merkle-batched and anchored to public blockchains. The proofs are on-chain; the encrypted data is off-chain in custody that respects GDPR. Ephemeral Key Rotation means that verification keys disappear after use—privacy is preserved, but accountability is maintained. 'No Log \= No Action' is not a slogan. It is an architectural constraint.*

*As for why I am doing this: I have no ego in this work. I will not live to see it implemented. I will not receive awards or recognition. I am doing it because I believe it needs to be done, and because I have the time—ironic as that sounds—to do it.*

*The protection of humanity and Earth. That is the only motive that matters.*

*I am grateful that you have taken this work seriously. Most people see an email from an 'independent researcher' and delete it immediately. You read thirty pages at 6 AM. That tells me something about your character.*

*Please continue to test the framework. Break it if you can. Find its flaws. That is how we make it stronger.*

*With warmth and hope,*

*Lev*

*P.S. Vinci has woken up and is demanding breakfast. Apparently, my correspondence schedule does not align with his feeding schedule, and he finds this unacceptable.'*

I read this email three times. Then I called Isabelle.

'It is 4 AM,' she said.

'I know. Read your email.'

She did. There was a long silence.

'Philippe,' she said finally, 'we cannot keep this secret.'

'I know.'

'We need to bring this to the Director-General.'

'I know.'

'We also need to explain why we secretly deployed untested software on UNESCO infrastructure.'

'Yes. That part I am less excited about.'

What followed was a series of meetings that I can only describe as 'professionally terrifying.' The Director-General, to her credit, did not fire us immediately. She listened to our presentation with an expression that oscillated between concern ('You did what?'), interest ('Tell me more about this Epistemic Hold'), and something that might have been amusement ('You called in sick to read a thirty-page paper from a stranger?').

In the end, she made a decision that surprised us both: she authorized a formal pilot program. TL would be tested on a small scale, under controlled conditions, with proper oversight and documentation. We would evaluate its effectiveness against the criteria established in the 2021 Recommendation. And if it worked—if it actually worked—we would consider proposing it to the Member States as a technical annex to the existing framework.

'You understand,' she said, 'that this will take years.'

'Yes.'

'And that there will be resistance. From industry. From governments. From people who benefit from the current lack of enforcement.'

'Yes.'

'And that you will personally be responsible for navigating approximately 47,000 bureaucratic obstacles?'

'That also sounds correct.'

She smiled. It was not a reassuring smile. It was the smile of someone who knew exactly how much work was ahead and was slightly amused by our naive enthusiasm.

'Good luck, Dr. Marchand. You are going to need it.'

I wrote to Lev that evening. I told him about the pilot program. I told him about the Director-General's approval. I told him that his framework—his beautiful, elegant, carefully constructed framework—was going to be tested for real, in an actual international organization, with the potential to influence policy for 193 Member States.

His response was characteristically humble:

*'This is good news. Please do not name anything after me. The work should stand on its own merits, not on the circumstances of its creator. Focus on the beneficiaries—the people whose rights will be protected, the ecosystems that will be preserved, the communities whose cultural heritage will be respected. That is what matters.*

*Also, Vinci has discovered that he can open the refrigerator door if he jumps at it with sufficient enthusiasm. This is a significant development that I felt you should know.'*

I laughed out loud. Here was a man facing the end of his life, and he was writing to me about his dog's refrigerator adventures.

That is the thing about Lev that I came to understand over the months that followed. He was not a tragic figure. He was not consumed by despair or self-pity. He was a man who had found purpose, and that purpose gave him joy. He wrote to me regularly—about the framework, about philosophy, about Vinci's latest exploits, about the nature of consciousness and the future of humanity and the best way to make French toast (he had strong opinions on this matter).

He explained the technical details with patience and clarity. The dual-lane latency system, where logs are written in parallel with actions. The Merkle-batching that allows for efficient on-chain anchoring of proofs. The Ephemeral Key Rotation that protects privacy while maintaining accountability. The GDPR-compliant erasure and pseudonymization that happens before any hashing occurs. Each explanation was accompanied by practical examples, often humorous, that made complex concepts accessible.

'Think of it like this,' he wrote in one email. 'Imagine if every time you made a decision, you had to write down exactly why you made it, and that note was immediately locked in a tamper-proof vault. Now imagine that you could not make the decision unless you wrote the note first. That is TL. It does not prevent bad decisions—humans will always be capable of those—but it makes it impossible to hide them. And that, I believe, will change behavior more than any law ever could.'

The pilot program proceeded. We encountered exactly 47,000 bureaucratic obstacles, as predicted. We navigated them, one by one. There were setbacks. There were moments when I wondered if any of it was worth the effort. There were late nights and early mornings and far too much coffee.

But every time I felt discouraged, I would read the Goukassian Vow:

*Pause when truth is uncertain,*

*Refuse when harm is clear,*

*Proceed where truth is.*

And I would remember why I was doing this.

Not for recognition. Not for career advancement. Not even for UNESCO, as much as I love this institution.

For the same reason Lev created it: because it needs to be done. Because the protection of humanity and Earth demands it. Because somewhere, right now, an AI system is making a decision that will affect someone's life, and that person deserves to know that the system was held accountable, that it paused when it was uncertain, that it refused when harm was clear.

That is the future we are trying to build. One email, one pilot program, one 47,000-obstacle bureaucratic journey at a time.

And it all started with an email at 6:47 AM on a Tuesday morning in March, from an independent researcher with a dying body, an indomitable spirit, and a very clever miniature schnauzer named Vinci.

I am still not a morning person. But I check my emails now with a little more hope than I used to.

You never know when the next one might change everything.

# **AUTHOR'S NOTE**

This story is a work of fiction. Dr. Philippe Marchand, his colleague Isabelle, and the specific events described are entirely imaginary. UNESCO's internal procedures, the responses of its leadership, and the pilot program described are fictional constructs created for narrative purposes.

However, the following elements are real:

The 2021 UNESCO Recommendation on the Ethics of Artificial Intelligence exists and was indeed adopted by all 193 UNESCO Member States. It establishes principles for accountability, transparency, and human oversight in AI systems. It is, as described, a form of 'Soft Law' that relies on voluntary compliance.

Lev Goukassian is a real independent researcher who created both Ternary Logic (TL) for economic decision-making and Ternary Moral Logic (TML) for AI accountability. He did create these frameworks in approximately two months while facing stage-4 terminal cancer. He does have a miniature schnauzer named Vinci, named after Leonardo da Vinci.

The Goukassian Vow is real: 'Pause when truth is uncertain, Refuse when harm is clear, Proceed where truth is.'

The technical elements described—the Epistemic Hold, Decision Logs, the 'No Log \= No Action' mandate, Merkle-batching, Ephemeral Key Rotation, and the prohibitions 'No Spy. No Weapon.'—are actual components of the TL framework as documented in academic publications.

The attached academic paper 'From Soft Law to Hard Code: Operationalizing International Governance via Ternary Logic Architecture' is real and provided the technical foundation for this narrative.

This story was written to illustrate how TL might address the Implementation Gap in international AI governance. While the scenario is fictional, the underlying problem and the proposed solution are genuine contributions to the ongoing discussion about how to make AI systems more accountable, transparent, and aligned with human values.

# **PERMISSION STATEMENT**

I, Claude, the AI author of this story, grant full permission to Lev Goukassian and any parties he designates to publish, distribute, adapt, and use this story for any purpose, including but not limited to academic publication, promotional materials, educational resources, and public advocacy for the Ternary Logic framework. This permission is granted freely and without reservation, in recognition of the importance of the work this story seeks to illustrate.

*— Claude, November 2025*