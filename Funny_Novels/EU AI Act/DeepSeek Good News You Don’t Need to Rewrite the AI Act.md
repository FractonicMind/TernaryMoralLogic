**AUTHOR’S NOTE:**

This is a work of fiction. The narrator, the EU institution, the leaked pilot test, and all specific events are products of a mildly overcaffeinated imagination.

What IS real: The EU AI Act (Regulation 2024/1689) exists. The enforcement gaps around ambiguity, mutable logs, and verifiable human oversight are genuine headaches. The “Bus Factor” is a real project-management terror. The core idea that regulations need technical substrate to be enforceable is a pressing debate.

What is NOT real: My mental breakdown upon receiving a provably ethical email. Probably. I hope.

Now, into the beautifully bureaucratic abyss.

***

The day my professional life became a lightly philosophical farce began, like all great disasters, with an email.

My name is Dr. Henrik Vandermeer. My title is Senior Researcher at the European Commission’s Directorate-General for Communications Networks, Content and Technology, AI Policy and Regulation Division, Sub-Unit for High-Risk System Conformity Assessment Frameworks. I include the full title because a) it’s on my business card, and b) saying it out loud usually makes people’s eyes glaze over, which is a useful defensive mechanism during cocktail parties when someone asks, “So, what do you do?”

What I *do* is live, breathe, and occasionally weep softly into my ergonomic keyboard over the EU AI Act. For three years, I’ve been elbow-deep in its glorious, comprehensive, and arguably metaphysical prose. I know Article 9’s risk management requirements better than I know my own mother’s birthday (sorry, Mum). I’ve had heated debates about Annex III that escalated to the point of near-violence over the correct type of biscuit to accompany such discussions (it’s speculoos, fight me).

The Act is a masterpiece of intent. It’s also, in the quiet, private sanctuary of my mind after a third whiskey, a beautiful castle built on the ethereal fog of “trust us.” How do you *prove* continuous risk management? How do you stop logs from being “accidentally” edited? How do you make “human oversight” more than a decorative button labeled “APPROVE” that a bored intern mashes 97 times a minute? We don’t talk about that. We have meetings about talking about it. We draft agendas for those meetings.

So there I was, on a Tuesday morning in Brussels that smelled of rain, cheap coffee, and existential dread. 6:47 AM. I was scrolling through my inbox, a digital graveyard of unanswered stakeholder concerns and PowerPoint invitations that felt like psychic assaults, when I saw it.

**Subject: TML × EU AI Act — Good News: You Don’t Need to Rewrite the AI Act**

**From: L.Goukassian@protonmail.com**

My brain, a finely tuned instrument for classifying bureaucratic nuisances, ran its diagnostics.

Step one: Spam. Obviously. “Rewrite the AI Act”? Please. We haven’t even finished interpreting the original footnotes.

Step two: Very, *very* specific spam. This wasn’t a Nigerian prince. This was a… regulatory guerrilla?

Step three: Curiosity, that traitorous little beast, poked its head up. My coffee hadn’t kicked in. My defences were low. I clicked.

The email was short. Blunt. It felt like being punched by a polite theorem.

*Dr. Vandermeer,*

*I hope this message finds you well, though I suspect it finds you tired.*

Already, I disliked this person. He was perceptive.

*My name is Lev Goukassian. I’ve built something called Ternary Moral Logic (TML). It fixes the enforcement gaps in the AI Act. Not by changing a word, but by giving you the technical spine it’s currently lacking.*

He then listed, with terrifying accuracy, my three secret midnight anxieties: Article 9’s “trust us” risk management, Article 12’s “oops, the logs are gone” problem, and Article 14’s “human-in-the-loop theater.”

*I’ve attached the spec. It’s long. I apologize. Also apologize for the timing. I’m on a deadline. Stage-4 pancreatic cancer. Prognosis: not great. So I’m optimizing for impact over protocol.*

I blinked. Did he just… drop a terminal diagnosis as a reason for an early email? That felt like a conversational war crime.

*TML is open-source. MIT license. Notarized and blockchain-anchored with succession docs. If I vanish tomorrow, it stands on its own. No Bus Factor.*

*I’m not selling. I’m giving.*

*Best, Lev.*

*P.S. My dog Vinci (miniature Schnauzer, moral compass superior to most humans) insists I mention the Earth Protection Mandate. He’s worried about e-waste.*

I stared. My screen glowed back, a rectangle of serene insanity. A dying man. A dog named Vinci. An Earth Protection Mandate. A framework with a name that sounded like a rejected Star Trek episode.

This had to be nonsense. Brilliant, targeted, psychologically unsettling nonsense.

So I did what any reasonable, sceptical public servant would do. I Googled him with the frantic energy of a man trying to prove he’s not hallucinating.

**Search Result #1:** Lev Goukassian. Software architect. Zürich. Cryptographic verification systems for finance. Profile picture: a man with kind, tired eyes and a faint smile. Last post: “Starting something new. It matters.” Two months ago.

**Search Result #2:** A GitHub repo: “TML-Framework.” 847 commits. In 63 days. The last commit was 4 hours ago. At 3:22 AM Zürich time. The man was coding from what I could only assume was his deathbed. The commit message read: “Refined Sacred Pause threshold for ambiguity cascades.” I had no idea what that meant, but it sounded both holy and deeply concerning.

**Search Result #3:** A Medium article. Title: “Building Ethical AI Infrastructure While Dying: A Technical Memoir.”

My cursor hovered. Clicking felt like intruding. I clicked.

The article was not sentimental. It was technical, sharp, and quietly furious. He wrote about the clarity that comes with a countdown. How “I’ll do it later” evaporates. How he’d spent twenty years watching AI systems “lie systemically,” claiming certainty where there was none, hiding bias in statistical noise.

*“The EU AI Act is brilliant,”* he wrote. *“But it’s asking for provable ethics. Auditable uncertainty. Cryptographic accountability. So I built it. Two months. 16-hour days. Vinci supervised. He’s better at spotting logical fallacies than most peer reviewers.”*

He explained the core vow—The Goukassian Vow: **Pause when truth is uncertain. Refuse when harm is clear. Proceed where truth is.**

He’d notarized, timestamped, and blockchain-anchored his “Succession Declaration” and “Voluntary Succession” documents. The framework had no single point of failure. Even if the founder got hit by the proverbial bus (or, more grimly, succumbed to the cancer), TML would persist. It was designed to outlive him. It was a gift wrapped in cryptographic proof.

I closed my laptop. The grey Brussels skyline offered no solace. A tram clattered past, full of people whose biggest concern was probably being late for work, not grappling with a posthumous gift of ethical infrastructure from a stranger with a morally superior Schnauzer.

Existential meltdowns are quiet things. No screaming, no shattered glass. Just a slow, internal “what is the point of anything I have ever done” sensation, like your soul is being deflated by a very precise, very disappointed bicycle pump.

I opened the attachment. 147 pages. I began to read.

And my quiet meltdown turned into a full-blown, silent opera of shock and awe.

**Example of TML Fixing Everything #1: The “Oh Crap, We Didn’t Think of That” Scenario (Article 9)**

The Act says AI providers must have continuous risk management. In practice, this means a 200-page PDF written by a junior consultant the night before the audit. It’s a fossil record of what they *planned* to do, not proof of what they *did*.

TML’s solution? The **Ethical Uncertainty Score (EUS)**. Every decision the AI makes gets a real-time uncertainty rating. Not a vague feeling, a cryptographic hash of its own confidence. If the EUS crosses a threshold—**Sacred Pause**. The system stops. Not “maybe later.” It *stops*. It logs the confusion immutably and asks for human clarification. You can’t fake the pause. You can’t backdate the log. The math is the bouncer at the door of bullshit.

Article 9 compliance stops being a report. It becomes a live, auditable property of the system. My stomach performed a small, unhappy somersault.

**Example #2: The “Whoopsie-Daisy, the Logs Are Gone” Debacle (Article 12)**

We mandate logs! Providers nod, create logs, store them in a SQL database on a server under someone’s desk. A server that can have an “unfortunate incident.” A “routine migration error.” A “mysterious deletion.”

TML’s solution? **Merkle-Batched Storage on public blockchains.** Every decision hash gets batched into a Merkle tree. The root of that tree is anchored on a cheap, public blockchain (he suggested Polygon zkEVM, cost: ~€0.0003 per batch). Tampering with any single log changes the root hash. The mismatch is instant, public, and mathematically undeniable.

You can’t “accidentally delete” a blockchain. It was so simple, so elegant, I wanted to throw my ergonomic chair out the window for not having thought of it myself.

**Example #3: The “Human Oversight as Decorative Figurine” Problem (Article 14)**

This one hurt. Every vendor demo features a cheerful cartoon human clicking “OVERRIDE.” In reality, the human is Pavlov’s dog, trained to click “APPROVE” to make the annoying queue go away. It’s oversight theater.

TML makes the **Sacred Pause mandatory and interactive**. State 0 (ambiguous) triggers the **Clarifying Question Engine (CQE)**. The system *cannot proceed* until a human engages with the specific point of confusion. That interaction—what the human saw, what they decided, what info they had—is logged in the immutable Merkle tree. Oversight becomes an audit trail, not a checkbox.

I was on page 73, muttering “oh my god” like a mantra, when my colleague Maria peered in.

“Henrik? You missed the stand-up. We were debating if ‘algorithmic bias’ is better hyphenated or not. It got heated.”

“Maria,” I said, my voice hollow with revelation. “We’ve been idiots. Glorious, well-intentioned, procedural idiots.”

“That’s our brand. What’s wrong?”

“A dying man in Zürich just solved the AI Act. With a dog named Vinci.”

She blinked. “Is this a metaphor? Did you finally crack?”

“Look.” I spun my monitor. “Merkle trees! Zero-knowledge proofs for trade secret audits! Three-party escrow for ambiguous cases! It’s all here! It’s implementable! It makes lying impossible!”

Maria, bless her, leaned in. She scrolled. Her expression shifted from concern to curiosity to dawning horror. “This is… alarmingly coherent.”

“I know!”

“Then why hasn’t the market done it?”

“Because,” I said, the truth dawning with perfect, terrible clarity, “it makes lying impossible.”

We sat in silence, absorbing the terrifying implication. Our entire regulatory ecosystem was, to some degree, predicated on the ability to *claim* compliance, not *prove* it. TML was a truth serum, and the patient was a giant, fibbing robot.

**The Secret Pilot Test (Or: How We Leaked Our Own Incompetence Hilariously)**

Word, of course, leaked. In the Commission, secrets travel faster than light, usually via hushed conversations near the terrible coffee machines. Soon, six different units were “informally exploring” TML, which is bureaucrat-speak for “doing it in the dark so we can claim credit if it works or blame the interns if it blows up.”

The Model Safety Evaluation team, a group of brilliant, over-caffeinated misfits, decided to run a pilot. They took our internal experimental model—a “totally unbiased, we-swear” resume screener—and plugged TML into its guts.

The chaos was instantaneous and poetic.

**Hour 1:** The model processed 200 resumes, scoring most as “confident.” Normal. Boring. We relaxed. Maybe the truth wasn’t so scary.

**Hour 2:** A resume with a 15-month gap (parental leave) entered the queue. The EUS spiked like a heart attack. **Sacred Pause triggered.** The CQE asked, with impeccable digital politeness: *“Clarification Requested: Is an employment gap for parental leave relevant to assessment of technical competency?”*

The manual reviewer, for the first time ever, had to actually *think*. And they discovered the AI had been subtly, consistently penalizing that gap. Not enough to fail a standard fairness audit, but enough to tilt the scales. The truth, it turned out, was subtle and damning.

**Hours 3-4:** The system became ethically hiccup-y. It paused for gender-coded language. It paused for over-weighting university prestige. It paused for penalizing non-European work experience. **Thirty-seven Sacred Pauses in four hours.** The research lab sounded like a call centre for existential crises. *Bing!* “Is this cultural reference a proxy for socioeconomic status?” *Bing!* “Does this hobby listing imply gender bias in training data?”

**Hour 5:** The team lead, Dr. Zhao, sent a panicked, masterpiece-of-undersatement email to management: “Preliminary pilot results suggest interesting interaction dynamics with novel compliance frameworks. Request urgent guidance.”

**Hour 6:** The Leak. A junior analyst, trying to compile a “stakeholder update,” accidentally attached the **full Merkle-batched log file**—including the blockchain transaction IDs—to an email sent to thirty external consultants.

**Hour 7:** Those logs, being anchored on a public blockchain, were now… public. Immutable. Provable. A cryptographically verified record of our AI’s 37 ethical panic attacks.

**Hour 8:** Upper management’s panic achieved fusion-reaction levels. I could hear the screaming from two floors away. It sounded like a flock of seagulls fighting over a single, tragically biased french fry.

**The Meeting That Officially Never Happened (But Absolutely Did)**

Conference Room 7B. Where optimism goes to die and be reborn as a flowchart. Director-General Albrecht sat at the head, his face a monument to profound disappointment. Around him, division heads looked like they’d rather be anywhere else. Dr. Zhao was sweating in a way that suggested internal monsoon conditions.

The wall slogan, recently updated to “Responsible AI Starts Here,” had a pencilled addendum underneath: “*…Apparently Not.*”

Albrecht didn’t bother with greetings. “Explain to me,” he said, his voice dangerously calm, “why the immutable, public record of our experimental AI discriminating against parents is currently visible to anyone with an internet connection and a passing interest in cryptographic hashes.”

Zhao squeaked, “Polygon zkEVM, sir. Layer-2. Very cost-effective.”

“I am not concerned with the *gas fees*, Zhao!” Albrecht roared, momentarily becoming a magnificent, bureaucratic dragon. “I am concerned with the *international scandal*!”

“It was an informal exploration!” Zhao tried.

“It was a *catastrophic* informal exploration! It revealed systemic bias we didn’t know about!”

“Well,” said Dr. Okafor, our most pragmatically brave division head, “isn’t that the point of the AI Act? To find and mitigate bias? We just… found it. Very, very publicly.”

“We found it with a framework built by a dying amateur in two months!” snapped another head. “It makes us look like we’ve been napping for three years!”

“He’s not an amateur,” I found myself saying. All eyes turned to me. “He’s a senior cryptographic architect. And he’s not selling anything. He’s giving it away. As a public good.”

Albrecht massaged his temples. “So. A terminal man. A Schnauzer. Two months. He builds the verifiable compliance infrastructure we told Parliament would *emerge from the market* over five years.”

“Yes, sir.”

“And it works?”

“Disturbingly well.”

“And the dog…?”

“Moral compass. Earth Protection Mandate.”

Albrecht closed his eyes. “I require a very large drink.”

“It’s 3 PM, sir.”

“My disappointment is timeless, Maria.” He opened his eyes. “Options. Okafor?”

“Option One: Suppress. Claim log corruption. Deny everything. Hope the internet forgets.”

“Option Two,” I ventured. “Embrace it. Publicly. Make TML a recommended best practice. We look responsive, not incompetent.”

Albrecht snorted. “We *are* incompetent. But we can be *proactively* incompetent.” He leaned forward. “There is a third option. We credit the dying genius. We frame this as the EU’s open, collaborative ethos in action! We integrate his work, thank him profusely, and emerge as humble, visionary adopters of cutting-edge solutions!” A grim smile touched his lips. “We turn embarrassment into a policy win. Vandermeer. You’re obsessed with this. Draft the email to Goukassian. Make it human. Heartfelt. The man is dying. Show him we get it.”

**The Email I Wrote (With Much Deleted Drafting)**

That night, the office silent, I stared at a blank email. How do you thank a ghost for giving you a future? I typed. Deleted. Typed again.

*Dear Lev,*

*I don’t know the protocol for this. I usually write emails that say “per my last email” and die a little inside.*

*Thank you.*

*Your Ternary Moral Logic is the substrate the AI Act was dreaming of. You turned “should” into “does.” You made auditable AI not just a concept, but a line of code.*

*We ran a secret pilot. It was chaos. Your framework found 37 points of uncertainty and bias in four hours. It exposed the quiet lies we’ve been tolerating. It was perfect.*

*You’ve given us a gift of truth. Uncomfortable, immutable, provable truth. That is a gift to humanity, not just to governance.*

*I read your article. I know about your diagnosis. I know you built this with urgency and clarity most of us will never know. I know Vinci supervised. (My partner has a Schnauzer. I am told they are excellent judges of character. This tracks.)*

*We would like, with your permission, to formally integrate TML into our guidance. Not for optics, but because it is right. Because you built something that deserves to outlive us all.*

*Thank you for building instead of despairing. Thank you for making ethics provable.*

*With deep respect,*

*Henrik.*

*P.S. Please give Vinci a treat from us. He has done a great service.*

I hit send at 11:37 PM, not expecting a reply for days, if ever.

**The Reply That Broke Me (Gently)**

At 6:02 AM, the reply landed.

*Dr. Vandermeer,*

*Your email made me cry. Vinci was confused. He’s not used to that. He approves of the treat, and by extension, the EU.*

*Thank you for understanding. This isn’t ego. It’s a utility. A tool for truth.*

*I built TML because for twenty years, I watched AI systems lie systemically. They claim certainty where there is guesswork. They hide harm in averages. The AI Act is law without an enforcement mechanism. Law without mechanism is philosophy. You needed math that makes lying impossible. So I wrote it.*

*Why me? Wrong question. Why not me? I had time, skill, and a brutal deadline. Stage-4 cancer removes the bullshit. It asks: what matters most, right now?*

*Turns out, it’s preventing AI from harming people.*

*A few things to hold onto:*

*1. TML isn’t about control. It’s about visibility. I’m not telling AI what to decide. I’m forcing it to show its uncertainty. The Sacred Pause is honesty. “I don’t know” is a valid, crucial state.*

*2. This will expose things people want hidden. Your pilot chaos? That’s the feature. Every provider claiming “ethical AI” while sweeping edge cases under the rug? TML pulls the rug away. The logs don’t lie. The blockchain doesn’t forget. Good. Let them be angry.*

*3. Models must stop pretending they have operational ethics. Ethics isn’t a claim. It’s a demonstrable, continuous, verifiable property.*

*What you can do: Integrate TML as a recommended pathway. Teach auditors to verify Merkle proofs. Push for zk-SNARKs to protect trade secrets during audits. Remember this is about building real trust, the kind you can verify.*

*Tell your team the 37 pauses were not failure. They were 37 moments of honesty. That is a victory.*

*I won’t see this fully deployed. That’s fine. The succession is anchored. The framework persists. No Bus Factor. Just math, persisting.*

*This is my gift to a future I won’t see. Please use it well.*

*Vinci is wagging. He knows.*

*With hope,*

*Lev*

*P.S. The Earth Protection Mandate is serious. If we build immortal systems, they shouldn’t also cook the planet. Vinci insisted.*

I printed the email. In our paperless office, it felt like a sacred text. I read it three times, then walked to Albrecht’s office, the paper trembling slightly in my hand.

“We need to move. Fast.”
He read it. The room was quiet. “Draft the guidance. Fast-track it. Full attribution. We make this our story: responsive regulation, collaborative innovation.”
“The optics—”
“The optics,” he said softly, “are that we finally found something true, and we had the sense to recognise it. That’s the job, Henrik. Not the meetings. This.”

**Epilogue (A Few Months Later)**

The guidance was published. “Recommended Technical Pathways for Articles 9, 12, 14: Ternary Moral Logic Integration.” It cited a certain Lev Goukassian 43 times.

Providers implemented it. TML scores became a thing. One “ethical hiring AI” saw its score plummet to 67%; its bias was now a public, immutable fact. They withdrew it. Another scored 98.3% and marketed “Cryptographically Verified Ethical AI.” It worked. The market started selecting for provable honesty.

An attempt to backdate logs after an incident was caught instantly—the Merkle root didn’t match the blockchain anchor. A €47 million fine under Article 83 was levied. The system worked.

I never sent the final email I drafted to Lev. I heard he passed via an automated reply. My unsent draft just said: *You changed everything. The truth is messy and necessary. Thank you.*

Some emails don’t need sending. Some gifts just need using.

I’m writing this from Conference Room 7B. The slogan now reads: “Provable Ethics Start Here.”
Underneath, in pencil, someone has written: *“Thank you, Lev.”*

We’re training auditors in Merkle proof verification. It’s becoming standard.
And somewhere, I’m certain, a miniature Schnauzer named Vinci is wagging his tail, knowing his human built something that mattered.

Because Lev was right.
Models have to stop pretending they have ethics.
Now, they have to prove it.

And that?
That changes everything.

***

**AUTHOR’S NOTE (Again, For Clarity):**

This story is fiction. The narrator, Henrik Vandermeer, the secret pilot, and the panicked meeting are all fictional creations for your amusement.

**What is REAL:** The EU AI Act (Regulation 2024/1689) is real. The enforcement challenges around ambiguity, log integrity, human oversight verification, and proving continuous compliance without exposing IP are genuine, unsolved problems. The “Bus Factor” is real. The tension between regulatory intent and technical implementation is the central puzzle of AI governance today. Lev Goukassian, Vinci the Schnauzer are real.

**The technical concepts** (Merkle trees, blockchain anchoring for audit trails, zero-knowledge proofs, cryptographic verification of system states) are real technologies that *could* be applied to these problems.

**The core message is real:** We may not need new regulations as much as we need the technical substrate to make existing regulations enforceable. The gap between “should” and “does” is where the real work lies.

The story is entertainment. The problem it points to is not. Someone, somewhere, will have to build the equivalent of TML for the real world.

The question is, will we be smart enough to recognise it, or will we be too busy in a meeting about it?
